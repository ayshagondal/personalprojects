# -*- coding: utf-8 -*-
"""Building a Demand Forecasting Model .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lC2GKUrlvRDYeJ-ucP_GguESzu09ZCTN
"""

# Import required libraries
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor
from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek
from pyspark.ml.feature import StringIndexer
from pyspark.ml.evaluation import RegressionEvaluator

# Initialize Spark session
my_spark = SparkSession.builder.appName("SalesForecast").getOrCreate()

# Importing sales data
sales_data = my_spark.read.csv(
    "Online Retail.csv", header=True, inferSchema=True, sep=",")

# Convert InvoiceDate to datetime
sales_data = sales_data.withColumn("InvoiceDate", to_date(
    to_timestamp(col("InvoiceDate"), "d/M/yyyy H:mm")))

# Group the sales data
grp_data = sales_data.groupBy("Country", "StockCode", "InvoiceDate", "Year", "Month", "Day", "Week", "DayOfWeek")

# Aggregate the grouped data to get the sum of Quantity and the average of UnitPrice
grp_data = grp_data.agg({"Quantity": "sum", "UnitPrice": "avg"})

# Rename the aggregated sum(Quantity) column to Quantity
grp_data = grp_data.withColumnRenamed("sum(Quantity)", "Quantity")

# Define the split date for training and testing datasets
split_date = "2011-09-25"

# Filter the data to create the training dataset (data before or on the split date)
train_data = grp_data.filter(col("InvoiceDate") <= split_date)

# Filter the data to create the testing dataset (data after the split date)
test_data = grp_data.filter(col("InvoiceDate") > split_date)

# Convert the training dataset to a Pandas DataFrame
pd_daily_train_data = train_data.toPandas()

# Index the Country column
country_indexer = StringIndexer(inputCol="Country", outputCol="CountryIndex").setHandleInvalid("keep")

# Index the StockCode column
stock_code_indexer = StringIndexer(inputCol="StockCode", outputCol="StockCodeIndex").setHandleInvalid("keep")

# Define the feature columns for the model
feature_cols = ["CountryIndex", "StockCodeIndex", "Month", "Year", "DayOfWeek", "Day", "Week"]

# Assemble the feature columns into a single feature vector
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# Define the RandomForestRegressor model
rf = RandomForestRegressor(featuresCol="features", labelCol="Quantity", maxBins=4000)

# Create a pipeline with the indexers, assembler, and model
pipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, rf])

# Fit the model on the training data
model = pipeline.fit(train_data)

# Make predictions on the test data
test_predictions = model.transform(test_data)

# Cast the prediction column to double
test_predictions = test_predictions.withColumn("prediction", col("prediction").cast("double"))

# Define the evaluator for Mean Absolute Error (MAE)
mae_evaluator = RegressionEvaluator(labelCol="Quantity", predictionCol="prediction", metricName="mae")

# Evaluate the model on the test data
mae = mae_evaluator.evaluate(test_predictions)

# identify specific week sales
weekly_test_predictions = test_predictions.groupBy("Year", "Week").agg({"prediction":"sum"})
# finding quantity of week 39 sales
promotion_week = weekly_test_predictions.filter(col('Week')==39)
quantity_sold_w39 = int(promotion_week.select("sum(prediction)").collect()[0][0])